mod.04 <- lm(I(Kerry04-Dean04) ~ diebold+PcntColl.Grad+unempRate+logPopD+logMedInc,na.action=na.exclude)
summary(mod.04)
summary(mod.05 <- lm(DV~diebold+PcntColl.Grad+unempRate+logPopD+logMedInc+I(Kerry04-Dean04),na.action=na.exclude))
summary(DieModel.5 <- lm(DV~diebold+PcntColl.Grad+unempRate+logPopD+lat*long,na.action=na.exclude))
#1c
hw1c #loess curve interpretation
# 1d
hw1d #linear line interp
source('~/Desktop/psyc5541/jrl/hw3/HW3.R')
knitr::opts_chunk$set(echo = TRUE)
#Import rtweet and other libs
library(rtweet)
library(dplyr)
# plotting and pipes - tidyverse!
library("ggplot2")
library(dplyr)
# text mining library
library(tidytext)
# plotting packages
library(igraph)
library(ggraph)
#Use our own token
twitter_token <- create_token(
app = 'meta_nlp',
consumer_key="pzOo1DHoCGpIjElgpHzVDHgl4",
consumer_secret="QpTDNngQEDv9CkroVTjBlpRQcoL4iJvGkrAqPNAmXrIFdEVSNE",
access_token="1301874142359293952-K7O1mzxSh7PEPR799S9Db3cfHzLx97",
access_secret="mOvMMSEmBGZwT9sqZzuX8Oe1K2PRFwohGEOA6KEkxXZ1w"
)
#search twitter
science_tweets <- search_tweets(q="computational+neuroscience", n = 18000, lang="en", include_rts = F)
head(science_tweets$text)
#lets do this manually & without the tidyverse package
science_tweets$stripped_text <- gsub("http.*","",  science_tweets$text)
science_tweets$stripped_text <- gsub("https.*","", science_tweets$stripped_text)
head(science_tweets$stripped_text)
#lets store it in a new column!
science_tweets_clean <- science_tweets%>%
dplyr::select(stripped_text)%>%
tidytext::unnest_tokens(word, stripped_text)
View(science_tweets_clean)
View(science_tweets)
View(science_tweets_clean)
# plot the top 15 words -- notice any issues?
science_tweets_clean %>%
count(word, sort = TRUE) %>%
top_n(15) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(x = word, y = n)) +
geom_col() +
xlab(NULL) +
coord_flip() +
labs(x = "Count",
y = "Unique words",
title = "Count of unique words found in tweets")
data("stop_words")
#what is it?
head(stop_words)
#how many rows w/ stop words
nrow(science_tweets_clean)
#remove
cleaned_tweet_words <- science_tweets_clean%>%
anti_join(stop_words)
#now how many rows?
nrow(cleaned_tweet_words)
View(science_tweets)
View(cleaned_tweet_words)
# plot the top 15 words -- notice any issues?
cleaned_tweet_words %>%
count(word, sort = TRUE) %>%
top_n(15) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(x = word, y = n)) +
geom_col() +
xlab(NULL) +
coord_flip() +
labs(x = "Count",
y = "Unique words",
title = "Count of unique words found in tweets")
# plot the top 15 words -- notice any issues?
cleaned_tweet_words %>%
count(word, sort = TRUE) %>%
top_n(15) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(x = word, y = n)) +
geom_col() +
xlab(NULL) +
coord_flip() +
labs(x = "Count",
y = "Unique words",
title = "Count of unique words found in tweets")
library(devtools)
install.packages("devtools")
library(devtools)
install_github("dgrtwo/widyr")
library(widyr)
# remove punctuation, convert to lowercase, add id for each tweet!
science_tweets_paired_words <- science_tweets %>%
dplyr::select(stripped_text) %>%
unnest_tokens(paired_words, stripped_text, token = "ngrams", n = 2)
science_tweets_paired_words %>%
count(paired_words, sort = TRUE)
library(igraph)
library(ggraph)
# plot comp neuro word network
# (plotting graph edges is currently broken)
science_words_counts %>%
filter(n >= 24) %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
# geom_edge_link(aes(edge_alpha = n, edge_width = n))
# geom_edge_link(aes(edge_alpha = n, edge_width = n)) +
geom_node_point(color = "darkslategray4", size = 3) +
geom_node_text(aes(label = name), vjust = 1.8, size = 3) +
labs(title = "Word Network: Tweets using the hashtag - Climate Change",
subtitle = "Text mining twitter data ",
x = "", y = "")
library(tidyr)
science_tweets_separated_words <- science_tweets_paired_words %>%
separate(paired_words, c("word1", "word2"), sep = " ")
science_tweets_filtered <- science_tweets_separated_words %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word)
# new bigram counts:
science_words_counts <- science_tweets_filtered %>%
count(word1, word2, sort = TRUE)
head(science_words_counts)
library(igraph)
library(ggraph)
# plot comp neuro word network
# (plotting graph edges is currently broken)
science_words_counts %>%
filter(n >= 24) %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
# geom_edge_link(aes(edge_alpha = n, edge_width = n))
# geom_edge_link(aes(edge_alpha = n, edge_width = n)) +
geom_node_point(color = "darkslategray4", size = 3) +
geom_node_text(aes(label = name), vjust = 1.8, size = 3) +
labs(title = "Word Network: Tweets using the hashtag - Climate Change",
subtitle = "Text mining twitter data ",
x = "", y = "")
#Import rtweet and other libs
library(rtweet)
library(dplyr)
# plotting and pipes - tidyverse!
library("ggplot2")
library(dplyr)
# text mining library
library(tidytext)
# plotting packages
library(igraph)
library(ggraph)
#Use our own token
twitter_token <- create_token(
app = 'meta_nlp',
consumer_key="pzOo1DHoCGpIjElgpHzVDHgl4",
consumer_secret="QpTDNngQEDv9CkroVTjBlpRQcoL4iJvGkrAqPNAmXrIFdEVSNE",
access_token="1301874142359293952-K7O1mzxSh7PEPR799S9Db3cfHzLx97",
access_secret="mOvMMSEmBGZwT9sqZzuX8Oe1K2PRFwohGEOA6KEkxXZ1w"
)
#search twitter
science_tweets <- search_tweets(q="#compneuro", n = 18000, lang="en", include_rts = F)
head(science_tweets$text)
#lets do this manually & without the tidyverse package
science_tweets$stripped_text <- gsub("http.*","",  science_tweets$text)
science_tweets$stripped_text <- gsub("https.*","", science_tweets$stripped_text)
head(science_tweets$stripped_text)
#lets store it in a new column!
science_tweets_clean <- science_tweets%>%
dplyr::select(stripped_text)%>%
tidytext::unnest_tokens(word, stripped_text)
# plot the top 15 words -- notice any issues?
science_tweets_clean %>%
count(word, sort = TRUE) %>%
top_n(15) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(x = word, y = n)) +
geom_col() +
xlab(NULL) +
coord_flip() +
labs(x = "Count",
y = "Unique words",
title = "Count of unique words found in tweets")
data("stop_words")
#what is it?
head(stop_words)
#how many rows w/ stop words
nrow(science_tweets_clean)
#remove
cleaned_tweet_words <- science_tweets_clean%>%
anti_join(stop_words)
#now how many rows?
nrow(cleaned_tweet_words)
# plot the top 15 words -- notice any issues?
cleaned_tweet_words %>%
count(word, sort = TRUE) %>%
top_n(15) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(x = word, y = n)) +
geom_col() +
xlab(NULL) +
coord_flip() +
labs(x = "Count",
y = "Unique words",
title = "Count of unique words found in tweets")
#Import rtweet and other libs
library(rtweet)
library(dplyr)
# plotting and pipes - tidyverse!
library("ggplot2")
library(dplyr)
# text mining library
library(tidytext)
# plotting packages
library(igraph)
library(ggraph)
#Use our own token
twitter_token <- create_token(
app = 'meta_nlp',
consumer_key="pzOo1DHoCGpIjElgpHzVDHgl4",
consumer_secret="QpTDNngQEDv9CkroVTjBlpRQcoL4iJvGkrAqPNAmXrIFdEVSNE",
access_token="1301874142359293952-K7O1mzxSh7PEPR799S9Db3cfHzLx97",
access_secret="mOvMMSEmBGZwT9sqZzuX8Oe1K2PRFwohGEOA6KEkxXZ1w"
)
#search twitter
science_tweets <- search_tweets(q="computational+neuroscience", n = 50000, lang="en", include_rts = F)
head(science_tweets$text)
nrow(science_tweets$text)
science_tweets$text
#lets do this manually & without the tidyverse package
science_tweets$stripped_text <- gsub("http.*","",  science_tweets$text)
science_tweets$stripped_text <- gsub("https.*","", science_tweets$stripped_text)
head(science_tweets$stripped_text)
#lets store it in a new column!
science_tweets_clean <- science_tweets%>%
dplyr::select(stripped_text)%>%
tidytext::unnest_tokens(word, stripped_text)
# plot the top 15 words -- notice any issues?
science_tweets_clean %>%
count(word, sort = TRUE) %>%
top_n(15) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(x = word, y = n)) +
geom_col() +
xlab(NULL) +
coord_flip() +
labs(x = "Count",
y = "Unique words",
title = "Count of unique words found in tweets")
data("stop_words")
#what is it?
head(stop_words)
#how many rows w/ stop words
nrow(science_tweets_clean)
#remove
cleaned_tweet_words <- science_tweets_clean%>%
anti_join(stop_words)
#now how many rows?
nrow(cleaned_tweet_words)
# plot the top 15 words -- notice any issues?
cleaned_tweet_words %>%
count(word, sort = TRUE) %>%
top_n(15) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(x = word, y = n)) +
geom_col() +
xlab(NULL) +
coord_flip() +
labs(x = "Count",
y = "Unique words",
title = "Count of unique words found in tweets")
library(devtools)
install_github("dgrtwo/widyr")
library(widyr)
# remove punctuation, convert to lowercase, add id for each tweet!
science_tweets_paired_words <- science_tweets %>%
dplyr::select(stripped_text) %>%
unnest_tokens(paired_words, stripped_text, token = "ngrams", n = 2)
science_tweets_paired_words %>%
count(paired_words, sort = TRUE)
library(tidyr)
science_tweets_separated_words <- science_tweets_paired_words %>%
separate(paired_words, c("word1", "word2"), sep = " ")
science_tweets_filtered <- science_tweets_separated_words %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word)
# new bigram counts:
science_words_counts <- science_tweets_filtered %>%
count(word1, word2, sort = TRUE)
head(science_words_counts)
library(igraph)
library(ggraph)
# plot comp neuro word network
# (plotting graph edges is currently broken)
science_words_counts %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
# geom_edge_link(aes(edge_alpha = n, edge_width = n))
# geom_edge_link(aes(edge_alpha = n, edge_width = n)) +
geom_node_point(color = "darkslategray4", size = 3) +
geom_node_text(aes(label = name), vjust = 1.8, size = 3) +
labs(title = "Word Network: Tweets using the hashtag - #compneuro",
subtitle = "Text mining twitter data ",
x = "", y = "")
library(igraph)
library(ggraph)
# plot comp neuro word network
# (plotting graph edges is currently broken)
science_words_counts %>%
filter(n >= 4) %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
# geom_edge_link(aes(edge_alpha = n, edge_width = n))
# geom_edge_link(aes(edge_alpha = n, edge_width = n)) +
geom_node_point(color = "darkslategray4", size = 3) +
geom_node_text(aes(label = name), vjust = 1.8, size = 3) +
labs(title = "Word Network: Tweets using the hashtag - #compneuro",
subtitle = "Text mining twitter data ",
x = "", y = "")
library(igraph)
library(ggraph)
# plot comp neuro word network
# (plotting graph edges is currently broken)
science_words_counts %>%
filter(n >= 3) %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
# geom_edge_link(aes(edge_alpha = n, edge_width = n))
# geom_edge_link(aes(edge_alpha = n, edge_width = n)) +
geom_node_point(color = "darkslategray4", size = 3) +
geom_node_text(aes(label = name), vjust = 1.8, size = 3) +
labs(title = "Word Network: Tweets using the hashtag - #compneuro",
subtitle = "Text mining twitter data ",
x = "", y = "")
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
#Use our own token
twitter_token <- create_token(
app = 'meta_nlp',
consumer_key="pzOo1DHoCGpIjElgpHzVDHgl4",
consumer_secret="QpTDNngQEDv9CkroVTjBlpRQcoL4iJvGkrAqPNAmXrIFdEVSNE",
access_token="1301874142359293952-K7O1mzxSh7PEPR799S9Db3cfHzLx97",
access_secret="mOvMMSEmBGZwT9sqZzuX8Oe1K2PRFwohGEOA6KEkxXZ1w"
)
#search twitter
news_tweets <- search_tweets("to:kompascom", n = 18000, include_rts = F)
#search twitter
news_tweets <- search_tweets("to:kompascom", n = 18000, include_rts = F)
#select columns that we'd like
data_fix <- news_tweets%>%
#remove duplicates
distinct(text, .keep_all = T)%>%
#take text only
select(created_at, text)
#create an id as the tweet identifier
data_fix["id"]<-1:nrow(data_fix)
#Convert when the tweet was created to the data format
data_fix$created_at <- as.Date(data_fix$created_at, format = "%Y-%m-%d")
#get text
text <- data_fix$text
#set the text to lowercase
text <- tolower(text)
#remove mentions, urls, emojis, numbers, punctuations, all that crap!
text <- gsub("@\\w+", "", text)
text <- gsub("https?://.+", "", text)
text <- gsub("\\d+\\w*\\d*", "", text)
text <- gsub("#\\w+", "", text)
text <- gsub("[^\x01-\x7F]", "", text)
text <- gsub("[[:punct:]]", " ", text)
#remove space and newlines
text <- gsub("\n", " ", text)
text <- gsub("^\\s+", "", text)
text <- gsub("\\s+$", "", text)
text <- gsub("[ |\t]+", " ", text)
#get it all into a new column
data_fix["fix_text"] <- text
head(data_fix$fix_text,100)
#get text
text <- data_fix$text
#set the text to lowercase
text <- tolower(text)
#remove mentions, urls, emojis, numbers, punctuations, all that crap!
text <- gsub("@\\w+", "", text)
text <- gsub("https?://.+", "", text)
text <- gsub("\\d+\\w*\\d*", "", text)
text <- gsub("#\\w+", "", text)
text <- gsub("[^\x01-\x7F]", "", text)
text <- gsub("[[:punct:]]", " ", text)
#remove space and newlines
text <- gsub("\n", " ", text)
text <- gsub("^\\s+", "", text)
text <- gsub("\\s+$", "", text)
text <- gsub("[ |\t]+", " ", text)
#get it all into a new column
data_fix["fix_text"] <- text
head(data_fix$fix_text,10)
library(tidytext)
#remove stopwords
stop_id <- scan(paste(getwd(), "/ID-stopwords.txt", sep=""), character(), sep = "\n")
setwd("~/Desktop/stat2600/report1")
library(tidytext)
#remove stopwords
stop_id <- scan(paste(getwd(), "/ID-stopwords.txt", sep=""), character(), sep = "\n")
# create df of stopwords
stop_words <- data.frame(
word <- stop_id,
straingsAsFactors = F
)
colnames(stop_words) <- "word"
#convert to tidy
tidy_text <- data_fix %>%
select(created_at, id, fix_text)%>%
#tokenize the word from the tweets
unnest_tokens(input=fix_text, output =word)%>%
#remove stop words
anti_join(stop_words, by="word")
tidy_text
library(rtweet)
library(dplyr)
#Use our own token
twitter_token <- create_token(
app = 'meta_nlp',
consumer_key="pzOo1DHoCGpIjElgpHzVDHgl4",
consumer_secret="QpTDNngQEDv9CkroVTjBlpRQcoL4iJvGkrAqPNAmXrIFdEVSNE",
access_token="1301874142359293952-K7O1mzxSh7PEPR799S9Db3cfHzLx97",
access_secret="mOvMMSEmBGZwT9sqZzuX8Oe1K2PRFwohGEOA6KEkxXZ1w"
)
#search twitter
news_tweets <- search_tweets("to:kompascom", n = 18000, include_rts = F)
#select columns that we'd like
data_fix <- news_tweets%>%
#remove duplicates
distinct(text, .keep_all = T)%>%
#take text only
select(created_at, text)
library(rtweet)
library(dplyr)
#Use our own token
twitter_token <- create_token(
app = 'meta_nlp',
consumer_key="pzOo1DHoCGpIjElgpHzVDHgl4",
consumer_secret="QpTDNngQEDv9CkroVTjBlpRQcoL4iJvGkrAqPNAmXrIFdEVSNE",
access_token="1301874142359293952-K7O1mzxSh7PEPR799S9Db3cfHzLx97",
access_secret="mOvMMSEmBGZwT9sqZzuX8Oe1K2PRFwohGEOA6KEkxXZ1w"
)
#search twitter
news_tweets <- search_tweets("to:kompascom", n = 18000, include_rts = F)
#select columns that we'd like
data_fix <- news_tweets%>%
#remove duplicates
distinct(text, .keep_all = T)%>%
#take text only
select(created_at, text)
library(rtweet)
library(dplyr)
#Use our own token
twitter_token <- create_token(
app = 'meta_nlp',
consumer_key="pzOo1DHoCGpIjElgpHzVDHgl4",
consumer_secret="QpTDNngQEDv9CkroVTjBlpRQcoL4iJvGkrAqPNAmXrIFdEVSNE",
access_token="1301874142359293952-K7O1mzxSh7PEPR799S9Db3cfHzLx97",
access_secret="mOvMMSEmBGZwT9sqZzuX8Oe1K2PRFwohGEOA6KEkxXZ1w"
)
#search twitter
news_tweets <- search_tweets("to:kompascom", n = 18000, include_rts = F)
#select columns that we'd like
data_fix <- news_tweets%>%
#remove duplicates
distinct(text, .keep_all = T)%>%
#take text only
select(created_at, text)
#create an id as the tweet identifier
data_fix["id"]<-1:nrow(data_fix)
#Convert when the tweet was created to the data format
data_fix$created_at <- as.Date(data_fix$created_at, format = "%Y-%m-%d")
#get text
text <- data_fix$text
#set the text to lowercase
text <- tolower(text)
#remove mentions, urls, emojis, numbers, punctuations, all that crap!
text <- gsub("@\\w+", "", text)
text <- gsub("https?://.+", "", text)
text <- gsub("\\d+\\w*\\d*", "", text)
text <- gsub("#\\w+", "", text)
text <- gsub("[^\x01-\x7F]", "", text)
text <- gsub("[[:punct:]]", " ", text)
#remove space and newlines
text <- gsub("\n", " ", text)
text <- gsub("^\\s+", "", text)
text <- gsub("\\s+$", "", text)
text <- gsub("[ |\t]+", " ", text)
#get it all into a new column
data_fix["fix_text"] <- text
head(data_fix$fix_text,10)
library(tidytext)
#remove stopwords
stop_id <- scan(paste(getwd(), "/ID-stopwords.txt", sep=""), character(), sep = "\n")
# create df of stopwords
stop_words <- data.frame(
word <- stop_id,
straingsAsFactors = F
)
colnames(stop_words) <- "word"
#convert to tidy
tidy_text <- data_fix %>%
select(created_at, id, fix_text)%>%
#tokenize the word from the tweets
unnest_tokens(input=fix_text, output =word)%>%
#remove stop words
anti_join(stop_words, by="word")
tidy_text
