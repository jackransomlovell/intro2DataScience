---
title: "preproc_and_sentiment_analysis_autonomic_meta"
author: "Jack Lovell"
date: "9/20/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# The meta-analysis of neuroimaging data
As an undergrad I have been lucky enough to work in Tor Wager's laboratory as a professional research assistant. Tor is one of the leaders of the field when it comes to fMRI. If you're interested in learning more about his research check out his website at: https://sites.dartmouth.edu/canlab/. One of the fields of research Tor is very familiar with is the meta-analysis of neuroimaging data.
Meta-analyses are particularly important in neuroimaging as they aggregate small (often underpowerd) datasets collected across many different laboratories. This technique allows us as researchers to conceptualize what is happening in the brain during a particular behavioral task much more effectivley. As the data are in a unified format opposed to being scaattered across laboratories with various different interpretations. 
This summer Tor announced to the lab that "he has decided it is time to do a meta-analysis of autonomic function." As someone who is interested in the process I volunteered as tribute, and boy did I not know what I was getting my self into. 

## Searching a database 
The first step in any meta-analysis is conducting your literature search. This is done after you and your team have decided on what terms you would like to include in your search. For us these terms included, but were not limited to: heart rate variablility (HRV), skin conductance response (SCR), and so forth. Many of the terms were common to psychophysiological experiments.
So when it came time to conduct our search, we knew it was going to be large (theres A LOT of neuorimaging studies that collect psychophys data). But upon completing our search in PubMed via Endnote, we had ~ 27,000 papers. The next few months were full of stressed late nights cutting the library down to a mear 1700 papers. Those 1700 papers are now being read and selected for the analysis based on if they include an analysis in their own study looking for linear relationships between physiological signals and changes in blood flow within the brain. But while sorting through said 27,000 abstracts and titles, I thought "can't an NLP algorithm do this?"
A quick search of the literature yielded an inspiring yes!
The same had been done in the genetic literature, where a large dataset (not quite as large) was analyzed with both an SVM and CNN to flag (not remove papers) as if they might be relevant or not for an analysis. Just what I was spending many hours doing! That dataset previously mentioned is still being worked on, so I will not be able to train a model for a couple of weeks still. 
BUT we can use techniques used in previous weekly reports to preprocess the data and start to look at the sentiments of the data! So let's get it crackin! We will use the tutorial found here: https://www.tidytextmining.com/index.html

# Cleaning our data for analysis

## The data
The data are in a .csv format available in this git repo. The only two fields we are interested in will be Title and Abstract, as they hold the most value in terms of text sentiment. Let's load the data in and take a look at them

```{r data}
library(dplyr)
ama_text <- read.csv('~/Desktop/report2/AMA.csv')
head(ama_text)
summary(ama_text)
```

Ok interesting, after reading the data in we have a a data frame with 2 variables "Title", and "Abstract" and ~ 27,000 rowds. Each row is actually a paper, and the informaiton that will let us know wether or not we would like to include that paper is in the abstract, so let's remove all rows without an abstract. 

```{r remove abstract.na}
ama_clean <- ama_text[!(ama_text$Abstract==""),]
head(ama_clean)
summary(ama_clean)
```

Great, that resulted in the removal of nearly 1,000 rows! Now, as in any analysis in R we will want to tidy our data! 

## Tidying text

Fortunately for us, there's an entire library useful for tidying text convienently named: 'tidytext'. Tidying text means we are left with a data frame of one token per row.

A token is a sentimental unit of text (i.e. a word) that could be used ofr analysis. Tokenization is the process of splitting text into tokens. For now we will tokenize abstracts and titles one at a time.


```{r tidytext}
library(tidytext)
tidy_title <- ama_clean %>%
  unnest_tokens(word,Abstract)
tidy_abstract <- ama_clean %>%
  unnest_tokens(word,Title)
summary(tidy_title)
summary(tidy_abstract)
```


## Stop Words

Great! Now that our data are tidy, let's remove those pesky stop words we learned about last time...

note - similar thing for heart related studies, we have a bunch of words we don't care about, they're stop words!! we can remove them. 

Lets actually use the features, then we can use naive-bayes or some other modeling technique

```{r stop words}
tidy_title <- tidy_title%>%
  anti_join(stop_words)
tidy_abstract <- tidy_abstract %>%
  anti_join(stop_words)
summary(tidy_title)
summary(tidy_abstract)
```

Great! Now we have two dataframes that are cleaned, tidy, and removed of stop words! Let's firgure out how we can compare the two and begin to visualize our data to understand them a bit better.

```{r compare}
library(ggplot2)
#lets look at the frequency in each first
tidy_title %>%
  count(word, sort = TRUE) %>%
  filter(n > 10000) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip()
tidy_abstract %>%
  count(word, sort = TRUE) %>%
  filter(n > 1000) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip()

```
Great! We can see there is a slight difference between the two groups, although for each these is a large number of words like "pulmonary" or "ventricular" it is likely these are related tpo purely heart studies. We will want to make note of this when we start to predict words. Let's now try to understand how the words in our text are interacting using bigrams

# n-grams

To predict whether or not a paper will be relevant for our meta-analysis, we will often want to understand how words in a given abstract will interact with eachother. Luckily in text analysis we can leverage some basic properties of probability to achieve this. 
The use of n-grams enables us to do so, as by splitting our text into n-grams, we can observe how often word X comes before word Y. Or vice-verse

Tidyttext makes this super easy, let's check it out under the abstracts!! 

```{r ngrams}

library(dplyr)
library(tidytext)

ama_bigrams <- ama_clean %>%
  unnest_tokens(bigram, Abstract, token = "ngrams", n = 2)

head(ama_bigrams)


#lets count the bigrams and filter them as well 

ama_bigrams %>%
  count(bigram,sort = T)

```


## Cleaning our bigrams
So obviously many of the stop words we removed from the last data.frames are popping up. Let's create two new columns to clean them up a bit. 

```{r clean}
library(tidyr)

bigrams_separated <- ama_bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ")

bigrams_filtered <- bigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

# new bigram counts:
bigram_counts <- bigrams_filtered %>% 
  count(word1, word2, sort = TRUE)

bigram_counts
```

Great! Now we have a new count of bigrams without those pesky stop words. Often we will want to visualize a network of which words occurs with eachother. We can easily do so in ggplot below! graph_to_data_fram() allows us to visualize "to" and "from" relations in our words using edges! 

```{r network}
library(igraph)
library(ggraph)

bigram_graph <- bigram_counts %>%
  filter(n > 1000) %>%
  graph_from_data_frame()

set.seed(2017)

ggraph(bigram_graph, layout = "fr") +
  geom_edge_link() +
  geom_node_point() +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1)


```

Interesting! There seems to be a lot of words relevant for things associated with the study of the heart. This is expected as in our search we paired the terms "fMRI" and ""heart" together. Which gave us almost the entirety of the literature from heart research. Next report we will focus on how to address this issue and hopefully get into some prediciton! 
